# La VoÃ»te Editor â€” DocumentaciÃ³n TÃ©cnica

> *El pipeline literario de 7 agentes LLM con interfaz web premium.*

---

## ğŸ“‹ Ãndice

- [Â¿QuÃ© es?](#quÃ©-es)
- [Â¿Por quÃ© existe?](#por-quÃ©-existe)
- [Arquitectura](#arquitectura)
- [CÃ³mo funciona](#cÃ³mo-funciona)
- [Archivos del proyecto](#archivos-del-proyecto)
- [Modelos LLM](#modelos-llm)
- [API Endpoints](#api-endpoints)
- [CÃ³mo levantar](#cÃ³mo-levantar)
- [CÃ³mo usar](#cÃ³mo-usar)
- [Historial de decisiones](#historial-de-decisiones)
- [Pendientes](#pendientes)

---

## Â¿QuÃ© es?

**La VoÃ»te Editor** es una interfaz web que orquesta 7 agentes LLM locales (vÃ­a Ollama) para generar relatos erÃ³ticos paso a paso, con intervenciÃ³n humana en cada checkpoint. Cada agente tiene un rol especÃ­fico y un system prompt dedicado.

## Â¿Por quÃ© existe?

### El problema
Originalmente se diseÃ±Ã³ un pipeline en **n8n** (herramienta de workflows visuales). Funcionaba, pero:
- La interfaz de n8n era tÃ©cnica y "fea" para un proceso creativo
- No permitÃ­a editar las salidas fÃ¡cilmente entre agentes
- Los checkpoints humanos eran incÃ³modos (nodos de "Wait" en n8n)

### La decisiÃ³n (27/Feb/2026)
Se decidiÃ³ construir una **interfaz web propia** con:
- Streaming en tiempo real (ver al agente "pensar")
- EdiciÃ³n directa del texto antes de aprobarlo
- DiseÃ±o premium con la estÃ©tica de La VoÃ»te (negro, oro, pÃºrpura)
- BotÃ³n para detener generaciÃ³n y guardar progreso en cualquier momento

### Â¿Por quÃ© Flask y no Node.js?
`npm` no estaba disponible en el sistema Windows del usuario. Python sÃ­ estaba instalado, asÃ­ que se usÃ³ **Flask** como backend. La elecciÃ³n fue pragmÃ¡tica, no arquitectÃ³nica.

---

## Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  USUARIO                       â”‚
â”‚         http://localhost:4000                  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚    Frontend (HTML/CSS/JS)         â”‚       â”‚
â”‚    â”‚    - index.html (estructura)      â”‚       â”‚
â”‚    â”‚    - style.css  (estÃ©tica VoÃ»te)  â”‚       â”‚
â”‚    â”‚    - app.js     (mÃ¡quina estados) â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚               â”‚ fetch() con streaming          â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚    Backend (Flask - server.py)     â”‚       â”‚
â”‚    â”‚    - Lee prompts de ../prompts/    â”‚       â”‚
â”‚    â”‚    - Proxy SSE a Ollama           â”‚       â”‚
â”‚    â”‚    - Guarda .md en 03_En_progreso â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚               â”‚ HTTP POST (stream: true)       â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚    â”‚    Ollama (Docker :11434)          â”‚       â”‚
â”‚    â”‚    - dolphin-mistral:7b           â”‚       â”‚
â”‚    â”‚    - dolphin-llama3:8b            â”‚       â”‚
â”‚    â”‚    - qwen2.5:7b                   â”‚       â”‚
â”‚    â”‚    - llama3.2:3b                  â”‚       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de datos

1. **Usuario** escribe premisa en textarea
2. **app.js** envÃ­a POST a `/api/agent/ideador` con la premisa
3. **server.py** carga `prompts/ideador.md` como system prompt
4. **server.py** envÃ­a a Ollama con `stream: true`
5. Ollama genera tokens uno a uno â†’ Flask los reenvÃ­a como SSE
6. **app.js** los muestra en el textarea de salida en tiempo real
7. **Usuario** edita, aprueba o regenera
8. El texto aprobado se usa como contexto para el siguiente agente
9. Repite para los 7 agentes

---

## CÃ³mo funciona

### Los 7 Agentes (en orden)

| # | Agente | Modelo | QuÃ© hace | System Prompt |
|---|--------|--------|----------|---------------|
| 1 | **Ideador** | dolphin-mistral:7b | Expande la premisa en una propuesta narrativa | `prompts/ideador.md` |
| 2 | **Arquitecto** | qwen2.5:7b | Estructura el arco argumental (3 actos, clÃ­max) | `prompts/arquitecto.md` |
| 3 | **Personajes** | dolphin-mistral:7b | Fichas psicolÃ³gicas, fÃ­sicas y de transformaciÃ³n | `prompts/personajes.md` |
| 4 | **Escritor** | dolphin-llama3:8b | Escribe el capÃ­tulo completo (mÃ­n 3000 palabras) | `prompts/escritor.md` |
| 5 | **CrÃ­tico** | qwen2.5:7b | EvalÃºa tensiÃ³n, ritmo, sensorialidad | `prompts/critico.md` |
| 6 | **Editor** | dolphin-llama3:8b | Aplica correcciones del crÃ­tico y reescribe | `prompts/editor.md` |
| 7 | **Contador** | llama3.2:3b | Verifica extensiÃ³n y formato final | `prompts/contador.md` |

### Checkpoints humanos

En **cada paso** el usuario puede:
- **Editar** el texto directamente en el textarea
- **Re-generar** si no le gusta la salida
- **Detener** la generaciÃ³n a mitad de camino (AbortController)
- **Guardar** el progreso actual como `.md`
- **Aprobar** y pasar al siguiente agente

### Agentes auto-invocados vs manuales

- **Manuales** (usuario debe hacer clic): Ideador, Escritor
- **Auto-invocados** (se disparan automÃ¡ticamente tras aprobar): Arquitecto, Personajes, CrÃ­tico, Contador

---

## Archivos del proyecto

```
web_interface/
â”œâ”€â”€ server.py              # Backend Flask (135 lÃ­neas)
â”‚   â”œâ”€â”€ MODELS{}           # Mapeo agente â†’ modelo Ollama
â”‚   â”œâ”€â”€ load_prompt()      # Lee ../prompts/{agente}.md
â”‚   â”œâ”€â”€ /api/agent/<name>  # Endpoint SSE streaming
â”‚   â””â”€â”€ /api/save          # Guarda estado como .md
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Estructura HTML (7 tarjetas, progress tracker)
â””â”€â”€ static/
    â”œâ”€â”€ style.css           # EstÃ©tica La VoÃ»te (Cinzel, gold, glassmorphism)
    â””â”€â”€ app.js              # MÃ¡quina de estados JS (streaming, abort, save)

prompts/                   # System prompts (archivos .md)
â”œâ”€â”€ ideador.md
â”œâ”€â”€ arquitecto.md
â”œâ”€â”€ personajes.md
â”œâ”€â”€ escritor.md
â”œâ”€â”€ critico.md
â”œâ”€â”€ editor.md
â””â”€â”€ contador.md
```

---

## Modelos LLM

### Â¿Por quÃ© Dolphin?

Los modelos estÃ¡ndar (Qwen, Llama) tienen filtros de seguridad que censuran contenido erÃ³tico explÃ­cito. Los modelos **Dolphin** son versiones "uncensored" entrenadas sin alignment restrictivo.

| Modelo | TamaÃ±o | Uso | RAM aprox |
|--------|--------|-----|-----------|
| `dolphin-mistral:7b` | 4.1 GB | Brainstorming y personajes | ~6 GB |
| `dolphin-llama3:8b` | 4.7 GB | Escritura y ediciÃ³n de prosa | ~7 GB |
| `qwen2.5:7b` | 4.7 GB | Estructura y anÃ¡lisis (no necesita uncensored) | ~6 GB |
| `llama3.2:3b` | 2.0 GB | Solo mÃ©tricas (ultraligero) | ~3 GB |

### ParÃ¡metros de generaciÃ³n

```python
"options": {
    "num_predict": 4096,      # 8192 para escritor/editor
    "temperature": 0.75,      # Creatividad moderada-alta
    "repeat_penalty": 1.3,    # Evita loops de texto repetido
    "repeat_last_n": 256      # Ventana de penalizaciÃ³n
}
```

---

## API Endpoints

### `GET /`
Sirve la interfaz web.

### `POST /api/agent/<agent_name>`
Genera texto con el agente especificado.

**Request:**
```json
{ "prompt": "Una mujer descubre que su espejo..." }
```

**Response:** Server-Sent Events (SSE)
```
data: {"token": "El", "done": false}
data: {"token": " espejo", "done": false}
...
data: {"token": ".", "done": true}
```

### `POST /api/save`
Guarda el progreso actual del pipeline.

**Request:**
```json
{
  "step": 3,
  "agent": "escritor",
  "premisa": "Una mujer descubre...",
  "context": {
    "ideador": "...",
    "arquitecto": "...",
    "personajes": "...",
    "escritor": "..."
  }
}
```

**Response:**
```json
{ "ok": true, "path": "03_Literatura/03_En_progreso/una_mujer_descubre_20260228_0930.md" }
```

---

## CÃ³mo levantar

### Requisitos previos
- Python 3.x con Flask y Requests instalados
- Docker Desktop corriendo
- Contenedor `voute_ollama` con los 4 modelos descargados

### Paso a paso

```powershell
# 1. Iniciar Docker Desktop (si no estÃ¡ corriendo)
Start-Process "C:\Program Files\Docker\Docker\Docker Desktop.exe"

# 2. Iniciar solo Ollama (los demÃ¡s servicios no son necesarios)
docker start voute_ollama

# 3. Iniciar el servidor Flask
cd C:\Users\fabara\LaVouteDAnais\web_interface
python server.py

# 4. Abrir en navegador
# http://localhost:4000
```

### Verificar que los modelos estÃ¡n disponibles
```powershell
docker exec voute_ollama ollama list
```
Debe mostrar: `dolphin-mistral:7b`, `dolphin-llama3:8b`, `qwen2.5:7b`, `llama3.2:3b`

---

## CÃ³mo usar

1. Abrir `http://localhost:4000`
2. Escribir la premisa en "Tu premisa oscura"
3. Clic en **"Invocar al Ideador"**
4. Ver cÃ³mo aparece el texto token por token
5. Editar el texto si se desea
6. **"Aprobar & Continuar â†’"** para avanzar (o **"â†» Re-generar"**)
7. El Arquitecto y Personajes se invocan automÃ¡ticamente
8. El Escritor requiere clic manual (es el mÃ¡s importante)
9. CrÃ­tico se auto-invoca, Editor requiere clic
10. Al final, **"ğŸ’¾ Guardar"** persiste todo en `03_En_progreso/`

### Tips
- Use **"â–  Detener"** si el agente se repite o pierde el rumbo
- Edite libremente el texto antes de aprobar â€” el siguiente agente usarÃ¡ SU versiÃ³n
- La primera generaciÃ³n es lenta (~2-3 min) porque Ollama carga el modelo en RAM

---

## Historial de decisiones

| Fecha | DecisiÃ³n | Motivo |
|-------|----------|--------|
| 27/Feb 16:14 | Pipeline en n8n con 14 nodos | Primera implementaciÃ³n rÃ¡pida |
| 27/Feb 17:30 | **Pivot a Web App custom** | n8n era tÃ©cnico, checkpoints incÃ³modos, sin ediciÃ³n directa |
| 27/Feb 17:45 | Flask en vez de Express | `npm` no disponible en Windows, Python sÃ­ |
| 27/Feb 20:24 | Agregar streaming SSE | Usuario querÃ­a ver al agente "pensar" en tiempo real |
| 27/Feb 20:24 | RediseÃ±o CSS completo | Primer diseÃ±o "aburrido", faltaba sello La VoÃ»te |
| 27/Feb 20:52 | `repeat_penalty: 1.3` | Arquitecto se repetÃ­a en loops |
| 27/Feb 20:54 | Botones Stop y Save | Usuario necesitaba control sobre generaciÃ³n y persistencia |
| 27/Feb 21:01 | Modelos Dolphin sin censura | Qwen/Llama censuran contenido erÃ³tico explÃ­cito |
| 28/Feb 09:29 | Puerto cambiado a 4000 | El 5000 conflictuaba con otro servicio |
| 28/Feb 09:44 | Look sin corsÃ© aceptado | DecisiÃ³n estilÃ­stica de la SeÃ±ora para looks deportivos |

---

## Pendientes

- [ ] Guardar tambiÃ©n en formato `.html` (usar Pandoc Docker)
- [ ] Implementar "Cargar sesiÃ³n guardada" (reanudar pipeline desde `.md`)
- [ ] Respetar el formato exacto de `ESTRUCTURA_MAESTRA_RELATOS.md`
- [ ] Agregar selector de "capÃ­tulo" para relatos multi-capÃ­tulo
- [ ] Crear script `.bat` para levantar todo con un clic
- [ ] Generar imÃ¡genes `side_profile` y `ditzy` pendientes cuando la API se libere
